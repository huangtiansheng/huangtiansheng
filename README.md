## Hi there ðŸ‘‹ I am Tiansheng Huang

- Iâ€™m currently a third-year PhD candidate from Georgia Tech.
- I am working on safety alignment for large language models. Particularly, I am interested in red-teaming attacks and defenses for LLMs.

## Selected Publications
I try to push myself to publish high quality papers in the periodicity of every three months. 

Here is the first paper I wrote in 2025 (maybe not only in 2025...)
- [2025/1/30]  **Virus: Harmful Fine-tuning Attack for Large Language Models bypassing Guardrail Moderation** *arXiv* [[paper](https://arxiv.org/abs/2501.17433)] [[code](https://github.com/git-disl/Virus)] 

Here are the papers I wrote in 2024. 
- [2024/9/26] **Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey** *arXiv* [[paper](https://arxiv.org/html/2409.18169v2)] [[repo](https://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers)]
- [2024/9/3] **Booster: Tackling harmful fine-tuning for large language models via attenuating harmful perturbation** *ICLR2025* [[paper](https://arxiv.org/abs/2409.01586)] [[code](https://github.com/git-disl/Booster)] [[Openreview](https://openreview.net/forum?id=tTPHgb0EtV)] 
- [2024/8/18] **Antidote: Post-fine-tuning safety alignment for large language models against harmful fine-tuning** *arXiv* [[paper](https://arxiv.org/abs/2408.09600)] 
- [2024/5/28] **Lazy safety alignment for large language models against harmful fine-tuning** *NeurIPS2024* [[paper](https://arxiv.org/abs/2405.18641)] [[code](https://github.com/git-disl/Lisa)]
- [2024/2/2] **Vaccine: Perturbation-aware alignment for large language model aginst harmful fine-tuning** *NeurIPS2024* [[paper](https://arxiv.org/abs/2402.01109)] [[code](https://github.com/git-disl/Vaccine)] 


