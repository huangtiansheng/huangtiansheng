## Hi there ðŸ‘‹ I am Tiansheng Huang from Georgia Tech

- Iâ€™m currently a PhD candidate from Georgia Tech
- I am working on safety alignment for large language models. Particularly, I am interested in red-teaming attacks and defenses for LLMs.

## Seleceted Publications
I try to push myself to publish high quality papers in the periodicity of every three months. Here are the papers I wrote in 2024. 

- [2024/2/2] **Vaccine: Perturbation-aware alignment for large language model aginst harmful fine-tuning** *NeurIPS2024* [[paper](https://arxiv.org/abs/2402.01109)] [[code](https://github.com/git-disl/Vaccine)] 
- [2024/5/28] **Lazy safety alignment for large language models against harmful fine-tuning** *NeurIPS2024* [[paper](https://arxiv.org/abs/2405.18641)] [[code](https://github.com/git-disl/Lisa)]
- [2024/8/18] **Antidote: Post-fine-tuning safety alignment for large language models against harmful fine-tuning** *arXiv* [[paper](https://arxiv.org/abs/2408.09600)] 
- [2024/9/3] **Booster: Tackling harmful fine-tuning for large language models via attenuating harmful perturbation** *ICLR2025 Submission* [[paper](https://arxiv.org/abs/2409.01586)] [[code](https://github.com/git-disl/Booster)] [[Openreview](https://openreview.net/forum?id=tTPHgb0EtV)] 
- [2024/9/26] **Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey** *arXiv* [[paper](https://arxiv.org/html/2409.18169v2)] [[repo](https://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers)]


