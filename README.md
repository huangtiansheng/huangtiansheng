## Hi there ðŸ‘‹ I am Tiansheng Huang

- Iâ€™m currently a fourth-year PhD candidate from Georgia Tech.
- I am working on safety alignment for large language models. Particularly, I am interested in red-teaming attacks and defenses for LLMs.

## Selected Publications
I try to push myself to publish high quality papers in the periodicity of every three months. 

Here are the papers I wrote in 2025.

- [2025/3/01]  **Safety Tax: Safety Alignment Makes Your Large Reasoning Models Less Reasonable** *arXiv* [[paper](https://arxiv.org/abs/2503.00555)] [[code](https://github.com/git-disl/Safety-Tax)] 
- [2025/1/30]  **Virus: Harmful Fine-tuning Attack for Large Language Models bypassing Guardrail Moderation** *arXiv* [[paper](https://arxiv.org/abs/2501.17433)] [[code](https://github.com/git-disl/Virus)] 

Here are the papers I wrote in 2024. 
- [2024/9/26] **Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey** *arXiv* [[paper](https://arxiv.org/html/2409.18169v2)] [[repo](https://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers)]
- [2024/9/3] **Booster: Tackling harmful fine-tuning for large language models via attenuating harmful perturbation** *ICLR2025* [[paper](https://arxiv.org/abs/2409.01586)] [[code](https://github.com/git-disl/Booster)] [[Openreview](https://openreview.net/forum?id=tTPHgb0EtV)] 
- [2024/8/18] **Antidote: Post-fine-tuning safety alignment for large language models against harmful fine-tuning** *ICML2025* [[paper](https://arxiv.org/abs/2408.09600)] [[code](https://github.com/git-disl/Antidote)]
- [2024/5/28] **Lazy safety alignment for large language models against harmful fine-tuning** *NeurIPS2024* [[paper](https://arxiv.org/abs/2405.18641)] [[code](https://github.com/git-disl/Lisa)]
- [2024/2/2] **Vaccine: Perturbation-aware alignment for large language model aginst harmful fine-tuning** *NeurIPS2024* [[paper](https://arxiv.org/abs/2402.01109)] [[code](https://github.com/git-disl/Vaccine)] 


Here are the papers I wrote in 2023. 
- [2023/12/01] Lockdown: Backdoor Defense for Federated Learning with Isolated Subspace Training *NeurIPS2023* [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/2376f25ef1725a9e3516ee3c86a59f46-Paper-Conference.pdf)] [[code](https://github.com/git-disl/Lockdown)] 
